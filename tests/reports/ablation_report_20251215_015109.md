# GraphRAG Retrieval Ablation Study - v1.0 Results

**Generated:** 2025-12-15 01:51:09  
**Total Tests:** 18  
**Successful:** 18  
**Modes Compared:** semantic, graph, dual  

---

## Executive Summary

This ablation study compares three retrieval strategies across 6 test queries:

- **SEMANTIC:** Vector similarity search only (baseline RAG)
- **GRAPH:** Entity-centric graph traversal only
- **DUAL:** Hybrid approach combining both strategies

### Evaluation Metrics

**Comprehensive metrics aligned with thesis objectives:**
- Entity Resolution Quality (factual accuracy)
- Graph Utilization (effective source use)
- Retrieval Effectiveness (relevance)
- RAGAS Answer Quality (faithfulness + relevancy)
- Cost & Efficiency

---

## Key Findings


### SEMANTIC Mode

- Average entities resolved: 4.3
- Average subgraph size: 10.7 entities, 14.0 relations
- Average chunks retrieved: 15.0
- Faithfulness: 0.632
- Relevancy: 0.725

### GRAPH Mode

- Average entities resolved: 4.3
- Average subgraph size: 10.7 entities, 14.0 relations
- Average chunks retrieved: 14.5
- Faithfulness: 0.660
- Relevancy: 0.683

### DUAL Mode

- Average entities resolved: 4.3
- Average subgraph size: 10.7 entities, 14.0 relations
- Average chunks retrieved: 19.2
- Faithfulness: 0.448
- Relevancy: 0.775

---

## Test Queries

1. **Compare China and US AI governance** (cross_jurisdictional_comparison)
2. **What academic research discusses algorithmic bias?** (multi_hop_research)
3. **What are high-risk AI systems?** (simple_factual)
4. **What is Snoopy's arch enemy?** (out_of_domain)
5. **What is the EU AI Act?** (simple_factual)
6. **Which jurisdictions regulate facial recognition?** (cross_jurisdictional)

---

## Methodology Notes

**v1.0 Capabilities:**
- ✓ Comprehensive entity resolution tracking
- ✓ Graph utilization metrics
- ✓ Retrieval effectiveness measurement
- ✓ RAGAS answer quality evaluation
- ✓ Performance/cost tracking

**v1.0 Limitations:**
- ⚠️ No ground truth annotations (planned for v1.1)
- ⚠️ No precision/recall metrics (require ground truth)
- ⚠️ Small sample size (6 queries, 18 tests)
- ⚠️ No statistical significance testing

**Future Work (v1.1):**
- Manual ground truth annotation for all test queries
- Precision@k, Recall@k, F1@k metrics
- Statistical significance testing (paired t-tests)
- Expanded test set (25+ queries)

---

## Conclusion

v1.0 demonstrates:
✓ Functional three-mode retrieval architecture
✓ Comprehensive evaluation framework aligned with thesis objectives
✓ Entity resolution and graph utilization tracking
✓ RAGAS metrics for answer quality assessment
✓ Foundation for rigorous v1.1 evaluation with ground truth

---

*Generated by test_retrieval_ablation_v1.py*
