# -*- coding: utf-8 -*-
"""
Phase 1C Threshold Refinement - Sample candidate pairs for human review.

Reads candidate pairs generated by disambiguation_processor (--phase faiss),
samples from different similarity bands for human labeling.

Input:  candidate_pairs.jsonl (from --phase faiss)
Output: 
  - threshold_samples.json (samples per band for labeling)
  - threshold_review.txt (human-readable format)

Workflow:
  1. python -m src.processing.entities.disambiguation_processor --phase faiss
  2. python -m src.processing.entities.tests.test_threshold_refinement
  3. Label pairs in threshold_review.txt
  4. Update thresholds in extraction_config.py
  5. python -m src.processing.entities.disambiguation_processor --phase full
"""

import json
import logging
import argparse
import random
from pathlib import Path
from typing import List, Dict

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Paths
DATA_DIR = Path('data')
PAIRS_FILE = DATA_DIR / 'interim' / 'entities' / 'candidate_pairs.jsonl'
SAMPLES_FILE = DATA_DIR / 'interim' / 'entities' / 'threshold_samples.json'

# Similarity bands for threshold tuning - finer granularity in critical zone
SIMILARITY_BANDS = [
    ('0.98+', 0.98, 1.01),
    ('0.96-0.98', 0.96, 0.98),
    ('0.94-0.96', 0.94, 0.96),
    ('0.92-0.94', 0.92, 0.94),
    ('0.90-0.92', 0.90, 0.92),
    ('0.88-0.90', 0.88, 0.90),
    ('0.85-0.88', 0.85, 0.88),
    ('<0.85', 0.70, 0.85),
]

# LLM cost estimates (from Phase 1D relation extraction)
# Mistral-7B via Together.ai: ~0.2s per call, $0.0002 per 1K tokens
LLM_SECONDS_PER_PAIR = 0.3  # Including network overhead
LLM_COST_PER_PAIR = 0.0001  # ~500 tokens per pair judgment


def analyze_similarity_distribution(pairs: List[Dict]) -> Dict:
    """Analyze similarity distribution of pairs."""
    if not pairs:
        return {'total_pairs': 0, 'bands': {}}
    
    sims = [p['similarity'] for p in pairs]
    
    # Count per band
    bands = {}
    for band_name, low, high in SIMILARITY_BANDS:
        count = sum(1 for s in sims if low <= s < high)
        bands[band_name] = count
    
    stats = {
        'total_pairs': len(pairs),
        'min_similarity': min(sims),
        'max_similarity': max(sims),
        'mean_similarity': sum(sims) / len(sims),
        'bands': bands,
    }
    
    return stats


def sample_pairs_for_tuning(pairs: List[Dict], samples_per_band: int = 15,
                            seed: int = 42) -> Dict[str, List[Dict]]:
    """
    Sample pairs from each similarity band for human review.
    
    Returns dict of band -> sample pairs for labeling.
    """
    random.seed(seed)
    
    # Group by band
    band_pairs = {band_name: [] for band_name, _, _ in SIMILARITY_BANDS}
    
    for pair in pairs:
        sim = pair['similarity']
        for band_name, low, high in SIMILARITY_BANDS:
            if low <= sim < high:
                band_pairs[band_name].append(pair)
                break
    
    # Sample from each band
    samples = {}
    for band_name in band_pairs:
        band_list = band_pairs[band_name]
        n_sample = min(samples_per_band, len(band_list))
        if n_sample > 0:
            samples[band_name] = random.sample(band_list, n_sample)
        else:
            samples[band_name] = []
    
    return samples


def format_samples_for_review(samples: Dict[str, List[Dict]]) -> str:
    """Format samples as human-readable text for labeling."""
    lines = []
    lines.append("="*70)
    lines.append("THRESHOLD TUNING - Label each pair: SAME / DIFF / UNSURE")
    lines.append("="*70)
    lines.append("")
    lines.append("Instructions:")
    lines.append("  SAME  = These refer to the same real-world entity")
    lines.append("  DIFF  = These are different entities")
    lines.append("  UNSURE = Cannot determine without more context")
    lines.append("")
    lines.append("Goal: Find thresholds where:")
    lines.append("  - auto_merge: lowest band where nearly ALL are SAME")
    lines.append("  - auto_reject: highest band where nearly ALL are DIFF")
    lines.append("  - uncertain band in between → send to LLM")
    
    for band_name, _, _ in SIMILARITY_BANDS:
        band_samples = samples.get(band_name, [])
        if not band_samples:
            continue
            
        lines.append(f"\n{'='*90}")
        lines.append(f"BAND: {band_name} ({len(band_samples)} samples)")
        lines.append("="*90)
        
        for i, pair in enumerate(band_samples, 1):
            # Handle both formats: entity1_key (list) or entity1_name (string)
            if 'entity1_key' in pair:
                name1 = pair['entity1_key'][0] if isinstance(pair['entity1_key'], list) else pair['entity1_key']
                type1 = pair['entity1_key'][1] if isinstance(pair['entity1_key'], list) else ''
                name2 = pair['entity2_key'][0] if isinstance(pair['entity2_key'], list) else pair['entity2_key']
                type2 = pair['entity2_key'][1] if isinstance(pair['entity2_key'], list) else ''
            else:
                name1 = pair.get('entity1_name', '')
                type1 = pair.get('entity1_type', '')
                name2 = pair.get('entity2_name', '')
                type2 = pair.get('entity2_type', '')
            
            # Truncate long names for side-by-side display
            name1_disp = name1[:35] + '..' if len(name1) > 37 else name1
            name2_disp = name2[:35] + '..' if len(name2) > 37 else name2
            type1_disp = type1[:12]
            type2_disp = type2[:12]
            
            lines.append(f"\n[{i:02d}] {pair['similarity']:.4f}  {name1_disp:<37} [{type1_disp:<12}]  vs  {name2_disp:<37} [{type2_disp:<12}]  → ___")
    
    lines.append("\n" + "="*90)
    lines.append("TALLY - Count per band after labeling:")
    lines.append("="*90)
    lines.append("")
    lines.append(f"{'Band':<12} {'SAME':>6} {'DIFF':>6} {'UNSURE':>6} {'%SAME':>8}")
    lines.append("-"*42)
    for band_name, _, _ in SIMILARITY_BANDS:
        lines.append(f"{band_name:<12} {'___':>6} {'___':>6} {'___':>6} {'___':>8}")
    lines.append("")
    lines.append("Recommended thresholds:")
    lines.append("  auto_merge_threshold:  ____ (lowest band where ~95%+ SAME)")
    lines.append("  auto_reject_threshold: ____ (highest band where ~95%+ DIFF)")
    lines.append("")
    lines.append("="*90)
    
    return "\n".join(lines)


def analyze_threshold_options(pairs: List[Dict]) -> str:
    """
    Generate table showing impact of different threshold choices.
    
    Shows for each potential auto_merge threshold:
    - Pairs auto-merged (no LLM)
    - Pairs in uncertain band (need LLM)
    - Estimated time and cost
    """
    lines = []
    lines.append("\n" + "="*90)
    lines.append("THRESHOLD ANALYSIS - Impact of different auto_merge thresholds")
    lines.append("="*90)
    lines.append("")
    lines.append("Assuming auto_reject = 0.85 (below this, pairs are clearly different)")
    lines.append(f"LLM estimates: {LLM_SECONDS_PER_PAIR}s/pair, ${LLM_COST_PER_PAIR}/pair")
    lines.append("")
    
    # Table header
    lines.append(f"{'Merge@':>8} {'Auto-Merge':>12} {'Uncertain':>12} {'Auto-Reject':>12} {'LLM Time':>12} {'LLM Cost':>10}")
    lines.append("-"*70)
    
    total = len(pairs)
    
    # Test different merge thresholds
    for merge_thresh in [0.98, 0.96, 0.94, 0.92, 0.90, 0.88, 0.85]:
        reject_thresh = 0.85
        
        auto_merge = sum(1 for p in pairs if p['similarity'] >= merge_thresh)
        auto_reject = sum(1 for p in pairs if p['similarity'] < reject_thresh)
        uncertain = total - auto_merge - auto_reject
        
        time_hrs = (uncertain * LLM_SECONDS_PER_PAIR) / 3600
        cost = uncertain * LLM_COST_PER_PAIR
        
        lines.append(f"{merge_thresh:>8.2f} {auto_merge:>12,} {uncertain:>12,} {auto_reject:>12,} {time_hrs:>10.1f}h ${cost:>9.2f}")
    
    lines.append("")
    lines.append("Note: Lower merge threshold = more auto-merges but higher false positive risk")
    lines.append("      Review samples above to find where SAME/DIFF boundary actually falls")
    lines.append("")
    
    return "\n".join(lines)


def load_pairs(filepath: Path) -> List[Dict]:
    """Load candidate pairs from JSONL."""
    pairs = []
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                pairs.append(json.loads(line))
    logger.info(f"Loaded {len(pairs)} pairs from {filepath}")
    return pairs


def main():
    parser = argparse.ArgumentParser(
        description='Sample candidate pairs for threshold tuning',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Workflow:
  1. Run: python -m src.processing.entities.disambiguation_processor --phase faiss
  2. Run: python -m src.processing.entities.tests.test_threshold_refinement
  3. Open threshold_review.txt and label pairs as SAME/DIFF/UNSURE
  4. Count results per band and determine thresholds
  5. Update extraction_config.py with new thresholds
  6. Run: python -m src.processing.entities.disambiguation_processor --phase full
"""
    )
    parser.add_argument('--pairs', type=str, default=str(PAIRS_FILE),
                       help='Input candidate pairs JSONL')
    parser.add_argument('--samples', type=int, default=15,
                       help='Samples per band for tuning')
    parser.add_argument('--seed', type=int, default=42,
                       help='Random seed for sampling')
    args = parser.parse_args()
    
    pairs_path = Path(args.pairs)
    
    # Check if pairs file exists
    if not pairs_path.exists():
        logger.error(f"Pairs file not found: {pairs_path}")
        logger.error("Run disambiguation_processor first:")
        logger.error("  python -m src.processing.entities.disambiguation_processor --phase faiss")
        return
    
    # Load pairs
    pairs = load_pairs(pairs_path)
    
    if not pairs:
        logger.error("No pairs found in file")
        return
    
    # Analyze distribution
    stats = analyze_similarity_distribution(pairs)
    
    print("\n" + "="*50)
    print("SIMILARITY DISTRIBUTION")
    print("="*50)
    print(f"Total pairs:    {stats['total_pairs']:,}")
    if stats['total_pairs'] > 0:
        print(f"Min similarity: {stats['min_similarity']:.4f}")
        print(f"Max similarity: {stats['max_similarity']:.4f}")
        print(f"Mean:           {stats['mean_similarity']:.4f}")
        print("\nBands:")
        for band_name, _, _ in SIMILARITY_BANDS:
            count = stats['bands'].get(band_name, 0)
            pct = count / stats['total_pairs'] * 100 if stats['total_pairs'] > 0 else 0
            print(f"  {band_name}: {count:>6,} ({pct:>5.1f}%)")
    
    # Sample for tuning
    samples = sample_pairs_for_tuning(pairs, samples_per_band=args.samples, seed=args.seed)
    
    # Generate threshold analysis
    threshold_analysis = analyze_threshold_options(pairs)
    print(threshold_analysis)
    
    # Save samples as JSON
    SAMPLES_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(SAMPLES_FILE, 'w', encoding='utf-8') as f:
        # Clean samples for JSON (remove idx)
        samples_clean = {}
        for band, pairs_list in samples.items():
            samples_clean[band] = [
                {k: v for k, v in p.items() if not k.startswith('idx')}
                for p in pairs_list
            ]
        json.dump(samples_clean, f, ensure_ascii=False, indent=2)
    logger.info(f"Saved samples to {SAMPLES_FILE}")
    
    # Save human-readable format (includes threshold analysis)
    review_file = SAMPLES_FILE.parent / 'threshold_review.txt'
    review_text = format_samples_for_review(samples)
    with open(review_file, 'w', encoding='utf-8') as f:
        f.write(review_text)
        f.write("\n")
        f.write(threshold_analysis)
    logger.info(f"Saved review file to {review_file}")
    
    # Print summary
    print("\n" + "="*50)
    print("OUTPUT FILES")
    print("="*50)
    print(f"Samples JSON:  {SAMPLES_FILE}")
    print(f"Review file:   {review_file}")
    print("\nNEXT: Open threshold_review.txt, label samples, pick thresholds")


if __name__ == '__main__':
    main()